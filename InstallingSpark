## Running Spark 2.0.0+ in Standalone mode

## For Mac Users 

Try Installing with Brew, this makes the life easy.
- Open your command line, if you have brew
      - update brew
  if you don't have brew,
      - Install brew, copy and paste the below command (Avialable at (http://brew.sh/))
      /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
      
- Install Hadoop , spark uses yarn resources and this is essentials 
       -> brew install hadoop 
  sometimes this will through an error if your system doesn't have java, cool thing about brew is it provides a command to 
  install java, run that command and java gets installed.
  
- Install spark 
      -> brew install apache-spark 
      
Success: Spark got installed. 
run pyspark on command line and you should see the spark running 


## Connecting ipython to spark 

I haven't followed the above process for working with spark on ipython notebook . I have uninstalled everything I have done 
before and worked on the following lines 

- Open the link (http://spark.apache.org/downloads.html) and download it in the folder you wish (in my case it is ~/Downloads)
- Go to the command line and execute the following the statements 
      *  tar xvzf spark-2.0.1-bin-hadoop2.7.tgz (unzip the downloaded folder )
      *  mv spark-2.0.1-bin-hadoop2.7 spark2 (change the filename to spark2)
      *  cd spark2/sbin (go to the sbin folder)
      * ./start-master.sh (open the start-master.sh file. This will start the spark 
      * cd ../logs
      * ls (should show you a link)
      * tail link(as given above)(this will show you that spark as started and give you web link to check the spark 

- In the command line 
      * pip install findspark (More about it https://github.com/minrk/findspark)
  
- In the ipython notebook 
     * import findspark 
     * findspark.init("/Users/Satish/Downloads/spark2/")
     * import pyspark 
     * from pyspark.sql import DataFrameNaFunctions 
     * from pyspark.sql.functions import lit 
     * from pyspark.ml.feature import StringIndexer #label encoding
     * from pyspark.ml import Pipeline
     * sc = pyspark.SparkContext(appName="helloworld")


## For Windows Users (Windows7/Windows10)
## Source::http://nishutayaltech.blogspot.in/2015/04/how-to-run-apache-spark-on-windows7-in.html

1. Java 
- Download & Install Java (latest version) through https://www.java.com/en/download/
- Add JAVA_HOME Path in System Variables
- Type "java -version" in command prompt to check whether it is installed properly or not

2. Scala
- Download and Install Scala (latest version) through http://www.scala-lang.org/download/
- Directly add <Scala location>\bin to the Path
- Type "scala -version" in command prompt to check whether it is installed properly or not

3. Spark
- We'll be using Spark prebuilt package for Hadoop 2.6
- Download Spark through this link:: https://spark.apache.org/downloads.html
- Extract the files to a folder (it can be your choice of folder)
- Add the folder path to SPARK_HOME (eg: SPARK_HOME = D:\Hadoop\spark-2.0.1-bin-hadoop2.6) to System Variables as "%SPARK_HOME%\bin"
- We need to download the 'winutils.exe' file and place it in a location (Link to download: https://github.com/steveloughran/winutils, clone the entire folder in your local system and unzip)
- Provide the path for HADOOP_HOME and add to system variables as "%HADOOP_HOME%\bin"
- Run "spark-shell" in the command prompt. In the Spark Shell, you would get the scala shell activate (scala>) which means that the installation is correctly done
- You can run various shells which includes the ones for Python (pyspark) and R Programming (SparkR) which will be helpful for Data Science Related tasks

4. Spark API and connecting it with Jupyter Notebooks
- You can run Python API on Spark by typing "pyspark" in the command prompt
- You'll get a message --> Using Python version 2.7.11 (default, Feb 16 2016 09:58:36) SparkSession available as 'spark'. which confirms the pyspark shell is running properly
- To connect it to the Jupyter Notebooks we need to set a few Paths in our Windows environment
	- PYSPARK_DRIVER_PYTHON = ipython
	- PYSPARK_DRIVER_PYTHON_OPTS = notebook
- Run the pyspark command once again in the command prompt and you'll be connected directly to the Jupyter notebooks (spark context would be automatically available through this)
- We can always create and import the pyspark module & sqlContext for our needs
